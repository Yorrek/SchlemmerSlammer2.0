{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "presiNLP.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FLdHlU0weiC",
        "colab_type": "text"
      },
      "source": [
        "# SchlemmerSlammer - Neural Network for recipes\n",
        "---\n",
        "## 1. Teammembers\n",
        "### Maxim Bex\n",
        "research, documentation, concept, coding\n",
        "### Hannes Gelbhardt\n",
        "coding, research, concept, documentation\n",
        "### York Smeddinck\n",
        "documentation, concept, coding, research\n",
        "\n",
        "---\n",
        "## 2. Problem Description\n",
        "We thought about training an AI to recommend / create recipes based on given attributes (e.g. rating, certain ingredients (include / exclude), calories, sodium, fat, FODMAP, etc.pp.). The following graphic is an entity relationship diagram for recipes, which we created to get a better understanding of the nature and complexity of our problem.\n",
        "\n",
        "![Entity Relationship Diagram for Recipes](https://i.imgur.com/SqWTTtY.png)\n",
        "\n",
        "Since this would be a rather complex system to begin with, we decided to break the problem down, and start with a smaller application first. Thus we are trying to train a neural network to recommend recipe ingredients based on given ingredients.\n",
        "\n",
        "\n",
        "### General Knowledge\n",
        "One of the more basic problems we face when we talking about natural language processing is that - in difference to humans - computers can just understand numbers, more specific ones and zeroes. It is not too hard to represent a language's syntax in numbers, but if we try to get meaning and context into language, humans are way better in understanding each other. The main reason for that is that humans often times have a similar context (background knowledge) so sometimes they even can communicate without words. To make this more clear, when we talk about cooking each human probably have a similar concept of preparing food in mind, but a computer just understands the word cooking out of it’s language context without actual knowing or understanding what it means.\n",
        "\n",
        "---\n",
        "## 3. Dataset\n",
        "### Source: https://www.kaggle.com/hugodarwood/epirecipes\n",
        "The Dataset is a collection of about 21000 recipes given in a .csv and .json format, where the json file contained all information of the recipes and the .csv file is basically a list of attributes and ingredients for all recipes. We opted for using the json file, which includes more context of the ingredients (e.g. 3 evenly chopped tomatoes) compared to the csv file (e.g. 3 tomatoes), which should result in a more complex model.\n",
        "### Processing the data:\n",
        "Because we put our focus on the relationship of ingredients, we preprocess the data from the json format into a textfile consisting of single line listings of the recipes ingredients, excluding special characters (see variable badchars in the code below.)\n",
        "\n",
        "The following script reads the input json file 'full_format_recipes.json', strips it of all unwanted characters defined in the variable 'badchars' and stores the result in the output file 'preProcessedData.txt', which we use for our mxnet neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnIsf70Bweib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing for our interpreter of the neural network\n",
        "\n",
        "import codecs\n",
        "import json\n",
        "\n",
        "badchars = set(\"(),-*\\\"'<>|:\")\n",
        "\n",
        "with open('full_format_recipes.json') as json_file:  \n",
        "    data = json.load(json_file)\n",
        "    outFile = \"./preProcessedData.txt\"\n",
        "    with codecs.open (outFile, \"w\", \"utf-8-sig\") as f:\n",
        "        for recipe in data:\n",
        "            try:\n",
        "                ingredients = recipe['ingredients']\n",
        "                for ingredient in ingredients:\n",
        "                    for c in badchars:\n",
        "                        ingredient = ingredient.replace(c,' ')\n",
        "                    try:\n",
        "                        f.write(ingredient+\" \")\n",
        "                    except UnicodeEncodeError:\n",
        "                        print(\"UnicodeEncodeError\")\n",
        "                f.write(\"\\n\")\n",
        "            except KeyError:\n",
        "                print(\"KeyError\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrwtRfXrY_rb",
        "colab_type": "text"
      },
      "source": [
        "The following script does basically the same, but since word2vec and glove need the items in a list of lists, it appends all ingredients of a recipe to an array (outRecipe), which is then appended to the output (outData).\n",
        "The result is an array of arrays (representing the recipes) including strings (the ingredients)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM1aInSmaFzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocessing for GloVe / Word2Vec\n",
        "#writing an array of arrays of ingredients in the output file\n",
        "\n",
        "import json\n",
        "import codecs\n",
        "\n",
        "badchars = set(\"(),-*\\\"'<>|:\")\n",
        "\n",
        "with open('full_format_recipes.json') as json_file:  \n",
        "    data = json.load(json_file)\n",
        "    outData = []\n",
        "    outFile = \"./dataset.dat\"\n",
        "    with codecs.open (outFile, \"w\", \"utf-8-sig\") as f:\n",
        "        for recipe in data:\n",
        "            outRecipe = []\n",
        "            try:\n",
        "                ingredients = recipe['ingredients']\n",
        "                for ingredient in ingredients:\n",
        "                    for c in badchars:\n",
        "                        ingredient = ingredient.replace(c,' ')\n",
        "                    try:\n",
        "                        outRecipe.append(ingredient)\n",
        "                    except UnicodeEncodeError:\n",
        "                        print(\"UnicodeEncodeError\")\n",
        "            except KeyError:\n",
        "                print(\"KeyError\")\n",
        "            outData.append(outRecipe)\n",
        "        f.write(\"%s\\n\" % outData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4RbysNxw2y",
        "colab_type": "text"
      },
      "source": [
        "### Problems during preprocessing\n",
        "The first problem was that the windows command line could not process specific characters of the dataset, which first was solved by setting the command line charset to \"UTF - 8\" Unfortunately we just could read and process about 700 recipes, but not all.\n",
        "\n",
        "Our solution to this was to do all the preprocessing through a python script (see below).\n",
        "\n",
        "Another problem were the unwanted “bad” characters (basically all symbols that are not letters) and how to exclude them.\n",
        "Our solution is to check every single word for said bad characters and to replace those with an empty space.\n",
        "\n",
        "Also we discussed whether to keep numbers (e.g. ¾) and measurement units (e.g. tbsp or cup), but we decided that our neural network would probably benefit from being able to put those aspects into context as well,so we kept them included. \n",
        "\n",
        "---\n",
        "##4. Neural Network(s)\n",
        "In our project we approched the problem in 3 ways.\n",
        "At first we implemented our own neural network utilizing mxnet / gluon (see 4.1.) but had problems with further application of our trained model (e.g. for T-SNE visualization or similar), then we gave GloVe (see 4.2.) a try, since it features a big pretrained vocabulary and offers easily accessible functionality like T-SNE visualization, and last but not least Word2Vec (see 4.3.) to get the accessible functionality like GloVe but without the pretrained vocabulary.\n",
        "\n",
        "###4.1. mxnet / gluon (our own approach)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A7_eI8jweir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import d2l\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "import pandas as pd\n",
        "\n",
        "import collections\n",
        "import math\n",
        "from mxnet import autograd, gluon, nd\n",
        "from mxnet.gluon import data as gdata, loss as gloss, nn\n",
        "import random\n",
        "import sys\n",
        "import collections\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldRTUT2Xwei7",
        "colab_type": "text"
      },
      "source": [
        "Read the number of lines to get an idea how many recipes we are working with"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ddwlRXgwei8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./preProcessedData.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    ingredients = [st.split() for st in lines]\n",
        "'# sentences: %d' % len(ingredients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfByZZdywejN",
        "colab_type": "text"
      },
      "source": [
        "To get a more meaningful result, delete the tokens, which come less then 3 times in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgJNZFzMwejQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counter = collections.Counter([tk for st in ingredients for tk in st])\n",
        "counter = dict(filter(lambda x: x[1] >= 3, counter.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF39pcp1wejb",
        "colab_type": "text"
      },
      "source": [
        "count the total number of tokens in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srD5whBdwejf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_to_token = [tk for tk, _ in counter.items()]\n",
        "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
        "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
        "           for st in ingredients]\n",
        "num_tokens = sum([len(st) for st in dataset])\n",
        "'# tokens: %d' % num_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ennJe2Ywejo",
        "colab_type": "text"
      },
      "source": [
        "To make the relationship between the most frequent words and the less frequent one more meaningful and have a better performance, we used subsampling and print out the result of the reduced dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYQ_naAmwejq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discard(idx):\n",
        "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
        "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
        "\n",
        "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
        "'# tokens: %d' % sum([len(st) for st in subsampled_dataset])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK94dQe-wejx",
        "colab_type": "text"
      },
      "source": [
        "For comparison we count the occurrence of the frequently used word \"cup\" before and after subsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lam-cF_Hwejz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_counts(token):\n",
        "    return '# %s: before=%d, after=%d' % (token, sum(\n",
        "        [st.count(token_to_idx[token]) for st in dataset]), sum(\n",
        "        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n",
        "\n",
        "compare_counts('cup')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgi-qBLCwej8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_centers_and_contexts(dataset, max_window_size):\n",
        "    centers, contexts = [], []\n",
        "    for st in dataset:\n",
        "        if len(st) < 2:\n",
        "            continue\n",
        "        centers += st\n",
        "        for center_i in range(len(st)):\n",
        "            window_size = random.randint(1, max_window_size)\n",
        "            indices = list(range(max(0, center_i - window_size),\n",
        "                                 min(len(st), center_i + 1 + window_size)))\n",
        "            indices.remove(center_i)\n",
        "            contexts.append([st[idx] for idx in indices])\n",
        "    return centers, contexts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSXFQkgVwekE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create artificilly a dataset with two random sentences of 2 to 7 words each\n",
        "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
        "print('dataset', tiny_dataset)\n",
        "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
        "    print('center', center, 'has contexts', context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpgWruojwekK",
        "colab_type": "text"
      },
      "source": [
        "In the following steps we implement the Skip-Gram Model. The first thing to do is trying to give the single words a context based meaning, so that in our previously giving example “I cook carrots” or “I cook potatoes” carrots, potatoes and cooking are closely related. On the top trough subsampling the performance of the later training part will be better, because the dataset is smaller"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvPBBeDowekM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test example, extracting words to the context max window size of 5\n",
        "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVItqiCvwekS",
        "colab_type": "text"
      },
      "source": [
        "Negative sampling is a second mechanism to imrpove the perfomance of the training part also as getter slightly better results. In the word representation after each  “going trough” the weights of all other words except the target word should be updated, which would need an enormous amount of compute power. In negative sampling we just pick a few, randomly chosen words which don’t occur in the context and change there weights related to the target word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33KLZv0iwekT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Negative sampling for training\n",
        "def get_negatives(all_contexts, sampling_weights, K):\n",
        "    all_negatives, neg_candidates, i = [], [], 0\n",
        "    population = list(range(len(sampling_weights)))\n",
        "    for contexts in all_contexts:\n",
        "        negatives = []\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "            if i == len(neg_candidates):\n",
        "                i, neg_candidates = 0, random.choices(\n",
        "                    population, sampling_weights, k=int(1e5))\n",
        "            neg, i = neg_candidates[i], i + 1\n",
        "            # Noise words cannot be context words\n",
        "            if neg not in set(contexts):\n",
        "                negatives.append(neg)\n",
        "        all_negatives.append(negatives)\n",
        "    return all_negatives\n",
        "\n",
        "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
        "all_negatives = get_negatives(all_contexts, sampling_weights, 5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wldWDiNpwekb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using mini batches gradient descent algorithm for the data reading process in a function \n",
        "def batchify(data):\n",
        "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
        "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
        "    for center, context, negative in data:\n",
        "        cur_len = len(context) + len(negative)\n",
        "        centers += [center]\n",
        "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
        "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
        "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
        "    return (nd.array(centers).reshape((-1, 1)), nd.array(contexts_negatives),\n",
        "            nd.array(masks), nd.array(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ3m9bWKwekh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use the previously defined batchify function to specify the data loader instance \n",
        "#and print the shape of each variable into the first batch read\n",
        "batch_size = 512\n",
        "#Checks how many cpu are available\n",
        "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "dataset = gdata.ArrayDataset(all_centers, all_contexts, all_negatives)\n",
        "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True,\n",
        "                             batchify_fn=batchify, num_workers=num_workers)\n",
        "#change of zip?\n",
        "for batch in data_iter:\n",
        "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
        "                           'labels'], batch):\n",
        "        print(name, 'shape:', data.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuyqykFfwekn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Skip-Gram Model\n",
        "#Embedding a layer with a input size of 20 neurons and a output of 4\n",
        "embed = nn.Embedding(input_dim=20, output_dim=4)\n",
        "embed.initialize()\n",
        "embed.weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT1bc6TXweky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The input of the embedding layer is the index of the context word\n",
        "x = nd.array([[1, 2, 3], [4, 5, 6]])\n",
        "embed(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRD0D9z_wek8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mini Batch Multiplication\n",
        "X = nd.ones((2, 1, 4))\n",
        "Y = nd.ones((2, 4, 6))\n",
        "nd.batch_dot(X, Y).shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me5iVTsDwelC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Skip-Gram Forward Calculation\n",
        "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
        "    v = embed_v(center)\n",
        "    u = embed_u(contexts_and_negatives)\n",
        "    pred = nd.batch_dot(v, u.swapaxes(1, 2))\n",
        "    return pred\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGZ_AyeMwelG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use Gluon's binary cross entropy loss function Sigmoid Binary Cross Entropy Loss\n",
        "loss = gloss.SigmoidBinaryCrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgq8Uou-welP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mask functions are considerable\n",
        "pred = nd.array([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
        "# 1 and 0 in the label variables label represent context words and the noise\n",
        "# words, respectively\n",
        "label = nd.array([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
        "mask = nd.array([[1, 1, 1, 1], [1, 1, 1, 0]])  # Mask variable\n",
        "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfQ_VjXUwelS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#binary cross-entropy loss function calculation to compare\n",
        "#and calculate the predicted value with a mask of 1 and the loss of the label based on the mask variable mask.\n",
        "def sigmd(x):\n",
        "    return -math.log(1 / (1 + math.exp(-x)))\n",
        "\n",
        "print('%.7f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4))\n",
        "print('%.7f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfSc_DoiwelY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Model Parameter with a size of 100, a sequential neural network\n",
        "embed_size = 100\n",
        "net = nn.Sequential()\n",
        "net.add(nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size),\n",
        "        nn.Embedding(input_dim=len(idx_to_token), output_dim=embed_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHKu3-vGweli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for the training process\n",
        "def train(net, lr, num_epochs):\n",
        "    ctx = d2l.try_gpu()\n",
        "    net.initialize(ctx=ctx, force_reinit=True)\n",
        "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
        "                            {'learning_rate': lr})\n",
        "    for epoch in range(num_epochs):\n",
        "        start, l_sum, n = time.time(), 0.0, 0\n",
        "        for batch in data_iter:\n",
        "            center, context_negative, mask, label = [\n",
        "                data.as_in_context(ctx) for data in batch]\n",
        "            with autograd.record():\n",
        "                pred = skip_gram(center, context_negative, net[0], net[1])\n",
        "                # Use the mask variable to avoid the effect of padding on loss\n",
        "                # function calculations\n",
        "                l = (loss(pred.reshape(label.shape), label, mask) *\n",
        "                     mask.shape[1] / mask.sum(axis=1))\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            l_sum += l.sum().asscalar()\n",
        "            n += l.size\n",
        "        print('epoch %d, loss %.2f, time %.2fs'\n",
        "              % (epoch + 1, l_sum / n, time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb2yU-RHwell",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(net, 0.005, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQVW2mlMwels",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    W = embed.weight.data()\n",
        "    x = W[token_to_idx[query_token]]\n",
        "    # The added 1e-9 is for numerical stability\n",
        "    cos = nd.dot(W, x) / (nd.sum(W * W, axis=1) * nd.sum(x * x) + 1e-9).sqrt()\n",
        "    topk = nd.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
        "    for i in topk[1:]:  # Remove the input words\n",
        "        print('cosine sim=%.3f: %s' % (cos[i].asscalar(), (idx_to_token[i])))\n",
        "\n",
        "get_similar_tokens('toasted', 10, net[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqb4sVX3welx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_file = datapath('./glove.6B.100d.txt')\n",
        "glove_file = './glove.6B.100d.txt'\n",
        "\n",
        "word2vec_glove_file = get_tmpfile(\"preProcessedData.txt\")\n",
        "word2vec_glove_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9BSdAUtwel3",
        "colab_type": "text"
      },
      "source": [
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l37KqT7hwel5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSSieMtawel-",
        "colab_type": "text"
      },
      "source": [
        "As we can see below, the trained model gives us pretty good results for word similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCn6cjxgwel_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.most_similar('tomato')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEmlQZIwemD",
        "colab_type": "text"
      },
      "source": [
        "In the following step we define a function to plot random samples via Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMzmNKFawemE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.vocab.keys()), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.vocab ]\n",
        "        \n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZffzSU1wemI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_pca_scatterplot(model, ['tomato','lettuce','onion','lamb','pork','salt','pepper','chopped','sliced','happy','beef','minced','flour','cake','bake','raw','italian','german','french','greek'], sample=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePLYz488wemO",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the figure the result gives us to our recipes unrelated words, which happened due to the base glove file which was made by a wikipedia entry crawler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GuEh8TKwemP",
        "colab_type": "text"
      },
      "source": [
        "Our Problem occured here is that we cant work with the previously trained network in the way, that we cannot use net.save, tsne or similar functions to word2vec"
      ]
    }
  ]
}